# XGBoost

For an XGBoost model to work we need to train it by using a datatype known as DMatrix.
Internally, the DMatrix includes:

-The feature matrix (in compressed format)(basically, what are the values of the various feature columns in the dataset)

-Labels (targets)(outputs)

Optional things like:

-Weights (importance per sample)

-Base margins

-Feature names

-Missing value masks


## ğŸš€ **Core XGBoost Hyperparameters â€“ Deep Dive**

---

### ğŸ”· 1. `max_depth` â€” *How deep each tree can go*

#### ğŸ” Intuition:

* Think of a tree asking questions like: â€œIs sepal length > 5.1?â€
* `max_depth` controls how many such decisions it can stack.
* More depth = more complex decision rules

#### ğŸ›  Practical Use:

* Small datasets â†’ `max_depth = 3â€“6`
* Large datasets â†’ `max_depth = 6â€“10` (but watch for overfitting)

#### ğŸ¯ Visual:

```python
max_depth=2
# Only 2 levels of splits: very simple trees, may underfit

max_depth=10
# Trees go very deep, can model complex patterns, but might memorize noise (overfit)
```

---

### ğŸ”· 2. `eta` or `learning_rate` â€” *How fast to learn*

#### ğŸ” Intuition:

* Each new tree tries to **fix the errors** of the previous trees.
* `eta` controls **how much of that fix is applied**.

#### ğŸ›  Practical Use:

* Lower `eta` = slower learning, but more stable.
* Common practice: `eta = 0.01 â€“ 0.3`
* If using small `eta`, increase `num_boost_round`

#### ğŸ¯ Visual:

```python
eta=0.3
# Big corrections â†’ fast learning â†’ risk of overshooting â†’ overfitting

eta=0.01
# Small corrections â†’ slow learning â†’ needs more trees but generalizes better
```

---

### ğŸ”· 3. `num_boost_round` â€” *Number of trees to grow*

#### ğŸ” Intuition:

* Each tree tries to correct the mistakes of the previous ones.
* More trees = better accuracy (up to a point)

#### ğŸ›  Practical Use:

* With low `eta`, youâ€™ll need more boosting rounds.

```python
# Tradeoff:
eta=0.3 â†’ 50 rounds might be enough
eta=0.05 â†’ might need 200+ rounds
```

---

### ğŸ”· 4. `subsample` â€” *What % of rows to use per tree*

#### ğŸ” Intuition:

* Like giving the model partial data each time (like Random Forests)
* Helps prevent overfitting

#### ğŸ›  Practical Use:

* Typical range: `0.5 â€“ 1.0`

```python
subsample=1.0
# Use full dataset each round â†’ more accurate but overfit risk

subsample=0.8
# Use 80% of data â†’ slightly noisier trees â†’ more generalizable
```

---

### ğŸ”· 5. `colsample_bytree` â€” *What % of features (columns) to use*

#### ğŸ” Intuition:

* Randomly selects a subset of features per tree â†’ improves robustness

#### ğŸ›  Practical Use:

* Try values from `0.5 to 1.0`

```python
colsample_bytree = 1.0
# Use all features each time

colsample_bytree = 0.7
# Use 70% of features â†’ prevents model from relying too much on 1 or 2 features
```

---

### ğŸ”· 6. `gamma` â€” *Minimum loss reduction to make a split*

#### ğŸ” Intuition:

* Controls tree **conservativeness**
* Higher gamma = model only makes splits that reduce error significantly

#### ğŸ›  Practical Use:

* Good for pruning
* Try values: `0, 1, 5, 10`

```python
gamma = 0
# Split freely to reduce error

gamma = 5
# Only split if it improves the objective a lot â†’ prevents overfitting
```

---

### ğŸ”· 7. `lambda` and `alpha` â€” *Regularization (like in Ridge/Lasso)*

| Parameter | Effect                       | Use When                  |
| --------- | ---------------------------- | ------------------------- |
| `lambda`  | L2 regularization on weights | Helps with collinearity   |
| `alpha`   | L1 regularization on weights | Can drive weights to zero |

#### ğŸ›  Practical Use:

* Use these if your model is **overfitting** or **too complex**

---

### ğŸ”· 8. `objective` â€” *Defines the problem type*

| Value              | Use Case                  |
| ------------------ | ------------------------- |
| `multi:softmax`    | Multiclass classification |
| `binary:logistic`  | Binary classification     |
| `reg:squarederror` | Regression                |

---

### ğŸ§ª Tuning Strategy:

1. **Start with default values**:

```python
params = {
  'objective': 'binary:logistic',
  'max_depth': 6,
  'eta': 0.3,
  'subsample': 1,
  'colsample_bytree': 1,
}
```

2. **If overfitting**:

   * Lower `max_depth`
   * Add `gamma`
   * Lower `eta`
   * Add regularization: `lambda` and `alpha`

3. **If underfitting**:

   * Increase `max_depth`
   * Reduce regularization
   * Increase `num_boost_round`

---

